dataset: d-vlog
trainer: temporal
log_every: 5

batch_size: 8
accumulation_steps: 1

model_checkpoint:
  monitor_quantity: MajorityClassificationEvaluator_f1
  direction: up

epochs: 50
eval_every: 1

evaluators:
  - name: majority_classification
    args:
      num_eval_runs: 10

heads:
  - kind: classification
    name: depression
    args:
      num_classes: 2

losses:
  - kind: xe
    name: depression
    target_head: depression
    args:
      num_classes: 2

seed: 42

# TODO sensitivity test
n_temporal_windows: 4
seconds_per_window: 5
presence_threshold: 0.25
max_audio_fps: 100
max_video_fps: 30

modalities:
  - name: audio_embeddings
    input_dim: 256
    model_args:
      latent_dim: 256

  - name: face_embeddings
    input_dim: 256
    model_args:
      latent_dim: 256

  # - name: face_landmarks
  #   input_dim: 204
  #   model_args:
  #     latent_dim: 256
  #     num_layers: 2
  #     self_attn_num_heads: 4
  #     self_attn_dim_head: 64
  #     dropout_rate: 0.1
  #     layer_dropout_rate: 0.0

  # - name: body_landmarks
  #   input_dim: 165
  #   model_args:
  #     latent_dim: 256
  #     num_layers: 2
  #     self_attn_num_heads: 4
  #     self_attn_dim_head: 64
  #     dropout_rate: 0.1
  #     layer_dropout_rate: 0.0

  # - name: hand_landmarks
  #   input_dim: 105
  #   model_args:
  #     latent_dim: 256
  #     num_layers: 2
  #     self_attn_num_heads: 4
  #     self_attn_dim_head: 64
  #     dropout_rate: 0.1
  #     layer_dropout_rate: 0.0

  # - name: gaze_features
  #   input_dim: 3
  #   model_args:
  #     latent_dim: 256
  #     num_layers: 2
  #     self_attn_num_heads: 4
  #     self_attn_dim_head: 64
  #     dropout_rate: 0.1
  #     layer_dropout_rate: 0.0

  # - name: blinking_features
  #   input_dim: 1
  #   model_args:
  #     latent_dim: 256
  #     num_layers: 2
  #     self_attn_num_heads: 4
  #     self_attn_dim_head: 64
  #     dropout_rate: 0.1
  #     layer_dropout_rate: 0.0

model: perceiver
model_args:
  latent_num: 16
  latent_dim: 128
  context_dim: 256
  cross_attn_num_heads: 4
  cross_attn_dim_head: 64
  cross_attn_parameter_sharing: false
  self_attn_block_type: "transformer"
  self_attn_num_layers: 12
  self_attn_num_heads: 4
  self_attn_dim_head: 64
  dropout_rate: 0.1
  layer_dropout_rate: 0.0
  self_attn_parameter_sharing: true
  extracting_embeddings: false

scheduler: "cyclelr"
scheduler_args:
  start_epoch: 0
  end_epoch: 300
  base_lr: 0.0001
  max_lr:  0.001
  mode: triangular
  step_size_up: 10
  step_size_down: 10

