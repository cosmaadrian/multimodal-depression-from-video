{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "import json\n",
    "from scipy.stats import mode\n",
    "import sklearn\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "## CHECK GROUP NAME ##\n",
    "######################\n",
    "exps_group = 'perceiver-edaic-ablation-modalities-big-arch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "########################################################################\n",
    "## COMPUTING PERFORMANCE METRICS USING DIFFERENT THRESHOLDS ##\n",
    "########################################################################\n",
    "def manipulate_predictions(df, source_dir, evaluate_dataset, min_threshold = 0.35, max_threshold = 0.75, simulate_no_thr = False):\n",
    "    new_df = defaultdict(list)\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        name = f\"temporal-evaluator:{row['name']}:over-time:{evaluate_dataset}.json\"\n",
    "        with open(f'../results/{source_dir}/{name}', 'rt') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        preds = []\n",
    "        true_labels = []\n",
    "        for key, value in data.items():\n",
    "            if key == 'preds_mean':\n",
    "                continue\n",
    "                \n",
    "            if key == 'preds_threshold_mean':\n",
    "                continue\n",
    "\n",
    "            the_preds = np.array(value['preds'])\n",
    "\n",
    "            if simulate_no_thr:\n",
    "                the_mode = mode(the_preds.round(), keepdims = True)[0][0]\n",
    "            else:\n",
    "                positive_predictions = np.ceil(the_preds[the_preds > max_threshold]).astype(np.int32)\n",
    "                negative_predictions = np.floor(the_preds[the_preds < min_threshold]).astype(np.int32)\n",
    "\n",
    "                tpreds = np.concatenate([positive_predictions, negative_predictions])\n",
    "\n",
    "                if not len(tpreds):\n",
    "                    the_mode = the_preds.mean() > 0.5 # if no preds, use mean\n",
    "                else:\n",
    "                    the_mode = mode(tpreds, keepdims = True)[0][0]\n",
    "\n",
    "            # if the_mode != value['true_label']:\n",
    "            #     print(key, the_mode, value['true_label'], the_preds.mean())\n",
    "\n",
    "            # get mode of preds\n",
    "            preds.append(the_mode)\n",
    "            true_labels.append(value['true_label'])\n",
    "\n",
    "        f1 = f1_score(true_labels, preds)\n",
    "        f1_weighted = f1_score(true_labels, preds, average = \"weighted\")\n",
    "        precision = precision_score(true_labels, preds)\n",
    "        precision_weighted = precision_score(true_labels, preds, average = \"weighted\")\n",
    "        recall = recall_score(true_labels, preds)\n",
    "        recall_weighted = recall_score(true_labels, preds, average = \"weighted\")\n",
    "        accuracy = accuracy_score(true_labels, preds)\n",
    "\n",
    "        new_df['f1'].append(f1)\n",
    "        new_df['f1_weighted'].append(f1_weighted)\n",
    "        new_df['precision'].append(precision)\n",
    "        new_df['precision_weighted'].append(precision_weighted)\n",
    "        new_df['recall'].append(recall)\n",
    "        new_df['recall_weighted'].append(recall_weighted)\n",
    "        new_df['accuracy'].append(accuracy)\n",
    "        new_df['seconds_per_window'].append(row['seconds_per_window'])\n",
    "        if 'modality' in row:\n",
    "            new_df['modality'].append(row['modality'])\n",
    "        new_df['run_id'].append(row['run_id'])\n",
    "\n",
    "    new_df = pd.DataFrame(new_df)\n",
    "    return new_df\n",
    "\n",
    "##################################\n",
    "## COLLECTING RESULTS FROM CSVs ##\n",
    "##################################\n",
    "def get_results(evaluate_dataset, evaluation_type = 'mode'):\n",
    "    dfs = []\n",
    "    result_files = glob.glob(f'../results/{exps_group}/*:{evaluate_dataset}.csv')\n",
    "\n",
    "    for file in result_files:\n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "        modality = file.split('/')[-1].split(':')[2]\n",
    "\n",
    "        if 'audiovisual' in modality:\n",
    "            df['modality'] = '3. audiovisual'\n",
    "        elif 'video' in modality:\n",
    "            df['modality'] = '2. video'\n",
    "        elif 'audio' in modality:\n",
    "            df['modality'] = '1. audio'\n",
    "        else:\n",
    "            print(file)\n",
    "            continue\n",
    "\n",
    "        dfs.append(df)\n",
    "\n",
    "    all_results = pd.concat(dfs, ignore_index=True).reset_index(drop = True)\n",
    "    results = all_results[(all_results['prediction_kind'] == evaluation_type)]\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " VALIDATION\n",
      "               f1_weighted           precision_weighted            \\\n",
      "                      mean       std               mean       std   \n",
      "modality                                                            \n",
      "1. audio          0.428197  0.254553           0.596522  0.313629   \n",
      "2. video          0.475312  0.302737           0.760079  0.067436   \n",
      "3. audiovisual    0.662101  0.051132           0.720157  0.024491   \n",
      "\n",
      "               recall_weighted           run_id  \n",
      "                          mean       std  count  \n",
      "modality                                         \n",
      "1. audio              0.453571  0.195044      5  \n",
      "2. video              0.517857  0.249042      5  \n",
      "3. audiovisual        0.650000  0.085267      5  \n",
      "\n",
      "\n",
      " TEST\n",
      "               f1_weighted           precision_weighted            \\\n",
      "                      mean       std               mean       std   \n",
      "modality                                                            \n",
      "1. audio          0.362790  0.172949           0.660419  0.122390   \n",
      "2. video          0.393048  0.220190           0.503775  0.257145   \n",
      "3. audiovisual    0.525327  0.094541           0.541544  0.045881   \n",
      "\n",
      "               recall_weighted           run_id  \n",
      "                          mean       std  count  \n",
      "modality                                         \n",
      "1. audio              0.414286  0.092272      5  \n",
      "2. video              0.478571  0.175600      5  \n",
      "3. audiovisual        0.539286  0.113220      5  \n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "###############################################\n",
    "## SHOWING RESULTS WITHOUT THRESHOLD OPTIMUM ##\n",
    "###############################################\n",
    "for evaluate_dataset in ['validation', 'test']:\n",
    "    dataset_results = get_results(evaluate_dataset)\n",
    "    manipulated_df = manipulate_predictions(dataset_results, exps_group, evaluate_dataset, simulate_no_thr = True)\n",
    "    to_print = manipulated_df.groupby('modality').agg({'f1_weighted': ['mean', 'std'], 'precision_weighted': ['mean', 'std'], 'recall_weighted': ['mean', 'std'], 'run_id': 'count'})\n",
    "    print('\\n\\n', evaluate_dataset.upper())\n",
    "    print(to_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [02:04<00:00, 15.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "BEST based on validation:\n",
      "0.45000000000000007 0.8000000000000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [02:10<00:00, 16.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "BEST based on test:\n",
      "0.45000000000000007 0.7500000000000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "##########################################################\n",
    "## EXPLORING DIFFERENT THRESHOLDS ACCORDING TO A METRIC ##\n",
    "##########################################################\n",
    "for evaluate_dataset in ['validation', 'test']:\n",
    "    global_sum_f1s = 0.0\n",
    "    best_grouped = None\n",
    "    best_max_threshold = 0.0\n",
    "    best_min_threshold = 1.0\n",
    "\n",
    "    dataset_results = get_results(evaluate_dataset)\n",
    "    for min_threshold in tqdm(np.arange(0.1, 0.5, 0.05)):\n",
    "        for max_threshold in tqdm(np.arange(0.5, 0.9, 0.05), leave = False):\n",
    "            manipulated_df =  manipulate_predictions(dataset_results, exps_group, evaluate_dataset, min_threshold, max_threshold)\n",
    "            grouped = manipulated_df.groupby('modality')\n",
    "            local_sum_f1s = grouped['f1_weighted'].mean().sum()\n",
    "\n",
    "            if local_sum_f1s > global_sum_f1s:\n",
    "                global_sum_f1s = local_sum_f1s\n",
    "                best_grouped = grouped\n",
    "                best_max_threshold = max_threshold\n",
    "                best_min_threshold = min_threshold\n",
    "            # print(min_threshold, max_threshold, grouped)\n",
    "\n",
    "    print(f'\\n\\nBEST based on {evaluate_dataset}:')\n",
    "    print(best_min_threshold, best_max_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 0.45 || 0.8\n",
      "\n",
      " VALIDATION\n",
      "               precision_weighted           recall_weighted            \\\n",
      "                             mean       std            mean       std   \n",
      "modality                                                                \n",
      "1. audio                 0.681783  0.048566        0.660714  0.177676   \n",
      "2. video                 0.704905  0.069459        0.492857  0.168748   \n",
      "3. audiovisual           0.752808  0.057458        0.635714  0.203054   \n",
      "\n",
      "               f1_weighted           run_id  \n",
      "                      mean       std  count  \n",
      "modality                                     \n",
      "1. audio          0.629872  0.155519      5  \n",
      "2. video          0.486705  0.153980      5  \n",
      "3. audiovisual    0.605040  0.220418      5  \n",
      "\n",
      " TEST\n",
      "               precision_weighted           recall_weighted            \\\n",
      "                             mean       std            mean       std   \n",
      "modality                                                                \n",
      "1. audio                 0.577181  0.126037        0.546429  0.113361   \n",
      "2. video                 0.524754  0.074653        0.453571  0.164441   \n",
      "3. audiovisual           0.531932  0.045585        0.550000  0.154338   \n",
      "\n",
      "               f1_weighted           run_id  \n",
      "                      mean       std  count  \n",
      "modality                                     \n",
      "1. audio          0.493066  0.120041      5  \n",
      "2. video          0.414597  0.171690      5  \n",
      "3. audiovisual    0.495639  0.141270      5  \n",
      "\n",
      "\n",
      " 0.35 || 0.85\n",
      "\n",
      " VALIDATION\n",
      "               precision_weighted           recall_weighted            \\\n",
      "                             mean       std            mean       std   \n",
      "modality                                                                \n",
      "1. audio                 0.673431  0.038147        0.653571  0.172023   \n",
      "2. video                 0.748885  0.093203        0.446429  0.190244   \n",
      "3. audiovisual           0.757271  0.062818        0.621429  0.201398   \n",
      "\n",
      "               f1_weighted           run_id  \n",
      "                      mean       std  count  \n",
      "modality                                     \n",
      "1. audio          0.624920  0.151434      5  \n",
      "2. video          0.412810  0.201765      5  \n",
      "3. audiovisual    0.595398  0.218152      5  \n",
      "\n",
      " TEST\n",
      "               precision_weighted           recall_weighted            \\\n",
      "                             mean       std            mean       std   \n",
      "modality                                                                \n",
      "1. audio                 0.579991  0.127145        0.546429  0.112656   \n",
      "2. video                 0.410622  0.207124        0.414286  0.177407   \n",
      "3. audiovisual           0.495553  0.044662        0.521429  0.160913   \n",
      "\n",
      "               f1_weighted           run_id  \n",
      "                      mean       std  count  \n",
      "modality                                     \n",
      "1. audio          0.494051  0.120854      5  \n",
      "2. video          0.347567  0.205705      5  \n",
      "3. audiovisual    0.461404  0.153226      5  \n"
     ]
    }
   ],
   "source": [
    "###################################################\n",
    "## SHOWING OBTAINED RESULTS FOR THE BEST SETTING ##\n",
    "###################################################\n",
    "# threholds based on validation set --> 0.45 0.80\n",
    "# thresholds based on test set      --> 0.45 0.75\n",
    "for min_thr, max_thr in [(0.45, 0.80), (0.35, 0.85)]:\n",
    "    print('\\n\\n', f'{min_thr} || {max_thr}')\n",
    "    for evaluate_dataset in ['validation', 'test']:\n",
    "        dataset_results = get_results(evaluate_dataset)\n",
    "        manipulated_df = manipulate_predictions(dataset_results, exps_group, evaluate_dataset, min_threshold = min_thr, max_threshold = max_thr)\n",
    "        to_print = manipulated_df.groupby('modality').agg({'precision_weighted': ['mean', 'std'], 'recall_weighted': ['mean', 'std'], 'f1_weighted': ['mean', 'std'], 'run_id': 'count'})\n",
    "        print('\\n', evaluate_dataset.upper())\n",
    "        print(to_print)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4667e081e7005511bcfee21a68bb8a28ab07bece98324499d02521cae4db1fea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
