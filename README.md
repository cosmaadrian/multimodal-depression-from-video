<h1 align="center"><span style="font-weight:normal">Reading Between the Frames</span><br>Multi-Modal Non-Verbal Depression Detection in Videos</h1>

<div align="center">
  
[📘Introduction](#intro) |
[🛠️Preparation](#preparation) |
[💪Training](#training) |
[🔮Evaluation](#evaluation) |
[📊Benchmark](#benchmark) |
[🐯Model Zoo](#model-zoo) |
[📝License](#license)
</div>

## Authors
<div align="center">
  
[Ana-Maria Bucur](https://scholar.google.com/citations?user=TQuQ5IAAAAAJ&hl=en), [Adrian Cosma](https://scholar.google.com/citations?user=cdYk_RUAAAAJ&hl=en), [David Gimeno-Gómez](https://scholar.google.es/citations?user=DVRSla8AAAAJ&hl=en), [Carlos-D. Martínez-Hinarejos](https://scholar.google.es/citations?user=M_EmUoIAAAAJ&hl=en), [Paolo Rosso](https://scholar.google.es/citations?user=HFKXPH8AAAAJ&hl=en)
</div>

````diff
- # TODO add abstract, diagram with method, main results, how to reproduce results, citation.
````

## <a name="intro"></a> Introduction
*Depression, a prominent contributor to global disability, affects a substantial portion of the population. Efforts to detect depression from social media text have been ongoing, yet relatively unexplored in the domain of user-generated video content. In this work, we address this research gap by proposing a novel multi-modal temporal model capable of discerning non-verbal depression cues from diverse modalities in noisy, real-world videos. The model uses seven non-verbal cues: audio speech embeddings, face emotion embeddings, face, body and hand landmarks and gaze and blinking information. Our model surpasses existing benchmarks by a substantial margin.*

## <a name="preparation"></a> Preparation


## <a name="training"></a> Training


## <a name="evaluation"></a> Evaluation


## <a name="benchmark"></a> Benchmark


## <a name="model-zoo"></a> Model Zoo


## <a name="license"></a> License
